{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: torch==1.8.1 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Collecting python-resize-image\n",
      "  Downloading python_resize_image-1.1.19-py2.py3-none-any.whl (8.4 kB)\n",
      "Requirement already satisfied: Pillow>=5.1.0 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from python-resize-image) (7.2.0)\n",
      "Requirement already satisfied: requests>=2.19.1 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from python-resize-image) (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from requests>=2.19.1->python-resize-image) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from requests>=2.19.1->python-resize-image) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from requests>=2.19.1->python-resize-image) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ndes9\\anaconda3\\lib\\site-packages (from requests>=2.19.1->python-resize-image) (2.10)\n",
      "Installing collected packages: python-resize-image\n",
      "Successfully installed python-resize-image-1.1.19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "!pip install torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "!pip install python-resize-image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pylab import *\n",
    "from resizeimage import resizeimage\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'tout'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'tout']\n"
     ]
    }
   ],
   "source": [
    "#create data\n",
    "root_dir=\"mnist_png\"\n",
    "transform2=transforms.Compose([transforms.Grayscale(1),transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data=ImageFolder(root=os.path.join(root_dir,\"training\"),transform=transform2)\n",
    "test_data=ImageFolder(root=os.path.join(root_dir,\"testing\"),transform=transform2)\n",
    "print(train_data.classes,test_data.classes)\n",
    "batch_size=50\n",
    "train11_loader=torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test11_loader =torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RN(torch.nn.Module):\n",
    "    def __init__(self,nbHiddenLayers,nbNeuronsPerLayer,n_in=28*28,n_out=11,): #3 couches cachees\n",
    "        super().__init__()\n",
    "        #fully connected layer\n",
    "        self.dim_in = n_in\n",
    "        self.dim_out = n_out\n",
    "        self.n_hidden_layers=nbHiddenLayers\n",
    "        self.nbNPerLayer=nbNeuronsPerLayer\n",
    "        #self.fc=None\n",
    "        if(self.n_hidden_layers > 0):\n",
    "            self.fc1 = torch.nn.Linear(self.dim_in,self.nbNPerLayer,bias=True)# In -> first hidden\n",
    "             # Hidden -> hidden\n",
    "            self.fc2=torch.nn.Linear(self.nbNPerLayer,self.nbNPerLayer,bias=True)\n",
    "            self.fc3=torch.nn.Linear(self.nbNPerLayer,self.dim_out,bias=True) # -> last hidden -> out\n",
    "        else:\n",
    "            self.fc1 = [torch.nn.Linear(self.dim_in,self.dim_out,bias=True)] # Single-layer perceptron\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fc=[self.fc1,self.fc2,self.fc3]\n",
    "        #fc=[self.fc1,self.fc2,self.fc3,self.fc4]\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            x=F.relu(fc[i](x))\n",
    "        x=fc[-1](x)\n",
    "        #return F.log_softmax(x,dim=1)\n",
    "        return F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(nbEntr,NN,loader,lr=1e-3):\n",
    "    start=time.time()\n",
    "    optimizer=torch.optim.Adam(NN.parameters(),lr) #lr : learning rate\n",
    "    lloss=[]\n",
    "    for i in range(nbEntr):\n",
    "        for x,y in loader:\n",
    "            NN.zero_grad() #\n",
    "            output=NN(x.view(-1,28*28))\n",
    "            loss=F.cross_entropy(output,y)\n",
    "            #loss=F.nll_loss(output,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        lloss.append(loss)\n",
    "    end=time.time()\n",
    "    print(\"Time execution of \", nbEntr ,\" trainings of \",end-start,\" s\" )\n",
    "    return lloss\n",
    "\n",
    "\n",
    "def test(loader,NN):\n",
    "    start=time.time()\n",
    "    correct_class =torch.zeros((11))\n",
    "    total=torch.zeros((11))\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            output=NN(x.view(-1,28*28))\n",
    "            for i,img in enumerate(output):\n",
    "                if(torch.argmax(img)==y[i]):\n",
    "                    correct+=1\n",
    "                    correct_class[y[i]]+=1\n",
    "                total[y[i]]+=1\n",
    "    end=time.time()\n",
    "    print(\"Time execution :  \",end-start)\n",
    "    return correct_class,total,correct/total.sum()\n",
    "\n",
    "def odd(correct,total):\n",
    "    res= []\n",
    "    for i in range(len(total)):\n",
    "        res.append(correct[i]/total[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 -> 0.37\n",
    "train0=ImageFolder(root=\"testN\",transform=transform2)\n",
    "loader0=torch.utils.data.DataLoader(train0, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgCam=ImageFolder(root=\"testC\",transform=transform2)\n",
    "imgCam_loader =torch.utils.data.DataLoader(imgCam, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbHiddenLayers = 2\n",
    "nbNeuronsPerLayer = 30\n",
    "myNN=RN(nbHiddenLayers,nbNeuronsPerLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution of  5  trainings of  0.33475160598754883  s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(2.3916, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3701, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3624, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3905, grad_fn=<NllLossBackward>),\n",
       " tensor(2.3301, grad_fn=<NllLossBackward>)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(5,myNN,loader0,lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution :   0.017626285552978516\n"
     ]
    }
   ],
   "source": [
    "c,t,s=test(loader0,myNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "[tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(1.), tensor(nan)]\n"
     ]
    }
   ],
   "source": [
    "print(s)\n",
    "print(odd(c,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution :   11.959380865097046\n"
     ]
    }
   ],
   "source": [
    "ct,tt,st=test(test11_loader,myNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1242)\n",
      "[tensor(0.9122), tensor(0.), tensor(0.), tensor(0.0248), tensor(0.), tensor(0.2321), tensor(0.), tensor(0.), tensor(0.1828), tensor(0.), tensor(0.)]\n"
     ]
    }
   ],
   "source": [
    "print(st)\n",
    "print(odd(ct,tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8115e-03, 2.9376e-03, 3.2019e-02, 5.3911e-03, 1.3177e-09, 8.2396e-01,\n",
       "        2.8342e-03, 2.7236e-03, 1.2923e-02, 3.2776e-05, 1.1437e-01],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"testC/0/a0.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.3637e-01, 1.2483e-02, 1.6958e-03, 6.8195e-04, 7.9221e-06, 7.2840e-03,\n",
       "        5.8710e-03, 2.2175e-02, 2.4016e-04, 7.7452e-05, 2.1312e-01],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"testC/0/a0.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0861, 0.1079, 0.0759, 0.0939, 0.0895, 0.1109, 0.0822, 0.0835, 0.0977,\n",
       "        0.0819, 0.0906], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image camera\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "#demo = Image.open(\"testC/1/ft1_425.png\") \n",
    "demo = Image.open(\"testC/5/ft5_444.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1445e-08, 2.7946e-10, 1.4167e-17, 2.1458e-17, 7.6985e-18, 2.6081e-14,\n",
       "        4.3537e-12, 1.8772e-11, 3.6164e-17, 1.4232e-15, 1.0000e+00],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image camera\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"testC/1/ft1_67.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3677e-09, 6.2508e-11, 4.1629e-19, 2.1297e-18, 1.8690e-19, 1.1170e-13,\n",
       "        2.6319e-12, 1.6533e-12, 3.9851e-18, 7.9422e-16, 1.0000e+00],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image camera\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"testC/8/ft8_67.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9879e-01, 1.0629e-05, 2.4153e-05, 1.2297e-07, 2.2967e-11, 9.3075e-05,\n",
       "        1.9679e-04, 1.2917e-04, 3.9680e-07, 9.4948e-09, 7.5951e-04],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image camera\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"mnist_png/testing/0/69.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9233e-06, 9.9347e-01, 3.4442e-05, 1.3838e-03, 2.8973e-06, 1.4669e-04,\n",
       "        1.1764e-03, 7.9726e-06, 1.0449e-03, 3.2773e-06, 2.7315e-03],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#image camera\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"mnist_png/testing/1/39.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1191, 0.0744, 0.1030, 0.0763, 0.1129, 0.0772, 0.1280, 0.0904, 0.0612,\n",
       "        0.0923, 0.0651], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo = Image.open(\"mnist_png/testing/2/905.png\") \n",
    "demo_img = trans(demo)\n",
    "myNN(demo_img.view(-1,28*28))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution of  5  trainings of  494.81303691864014  s\n",
      "Time execution :   112.87179493904114\n",
      "tensor(0.9643)\n",
      "[tensor(0.9829), tensor(0.9869), tensor(0.9491), tensor(0.9551), tensor(0.9577), tensor(0.9598), tensor(0.9863), tensor(0.9694), tensor(0.9363), tensor(0.9509), tensor(0.9950)]\n"
     ]
    }
   ],
   "source": [
    "#nbHiddenLayers = \n",
    "#nbNeuronsPerLayer = 15\n",
    "#myNN=RN(nbHiddenLayers,nbNeuronsPerLayer)\n",
    "train(5,myNN,train11_loader,lr=1e-3)\n",
    "c,t,s=test(train11_loader,myNN)\n",
    "print(s)\n",
    "print(odd(c,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution :   20.474367380142212\n",
      "tensor(0.9580)\n",
      "[tensor(0.9806), tensor(0.9921), tensor(0.9341), tensor(0.9644), tensor(0.9542), tensor(0.9451), tensor(0.9729), tensor(0.9562), tensor(0.9251), tensor(0.9376), tensor(0.9840)]\n"
     ]
    }
   ],
   "source": [
    "ct,tt,st=test(test11_loader,myNN)\n",
    "print(st)\n",
    "print(odd(ct,tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image de testC\n",
    "for i in imgCam_loader:\n",
    "    x,y=i\n",
    "    output=myNN(x.view(-1,28*28))\n",
    "    for i,img in enumerate(output):\n",
    "        plt.imshow(x[i].view(28,28))\n",
    "        plt.title(\"Predicted class {}\".format(torch.argmax(myNN(x[i].view(-1,28*28))[0])))\n",
    "        plt.show()\n",
    "        print((myNN(x[i].view(-1,28*28))[0]))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
